{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f08b15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Loss: 0.2581, Train Acc: 1.0000, Val Acc: 0.7920, Test Acc: 0.8160\n",
      "Epoch 040, Loss: 0.0548, Train Acc: 1.0000, Val Acc: 0.7720, Test Acc: 0.7970\n",
      "Epoch 060, Loss: 0.0339, Train Acc: 1.0000, Val Acc: 0.7660, Test Acc: 0.7890\n",
      "Epoch 080, Loss: 0.0429, Train Acc: 1.0000, Val Acc: 0.7680, Test Acc: 0.8000\n",
      "Epoch 100, Loss: 0.0420, Train Acc: 1.0000, Val Acc: 0.7720, Test Acc: 0.7970\n",
      "Epoch 120, Loss: 0.0344, Train Acc: 1.0000, Val Acc: 0.7660, Test Acc: 0.7960\n",
      "Epoch 140, Loss: 0.0267, Train Acc: 1.0000, Val Acc: 0.7660, Test Acc: 0.7960\n",
      "Epoch 160, Loss: 0.0257, Train Acc: 1.0000, Val Acc: 0.7640, Test Acc: 0.7930\n",
      "Epoch 180, Loss: 0.0221, Train Acc: 1.0000, Val Acc: 0.7660, Test Acc: 0.7970\n",
      "Epoch 200, Loss: 0.0317, Train Acc: 1.0000, Val Acc: 0.7680, Test Acc: 0.7990\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 0. Dependências\n",
    "# -----------------------------\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Carregar Dataset\n",
    "# -----------------------------\n",
    "os.makedirs(\"../datasets\", exist_ok=True)\n",
    "dataset = Planetoid(root=\"../datasets/Cora\", name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Definir Modelo GCN\n",
    "# -----------------------------\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Funções de treino e teste\n",
    "# -----------------------------\n",
    "def train(gcn, optimizer, data):\n",
    "    gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = gcn(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(gcn, data):\n",
    "    gcn.eval()\n",
    "    out = gcn(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        acc = accuracy_score(data.y[mask].cpu(), pred[mask].cpu())\n",
    "        accs.append(acc)\n",
    "    return accs  # [train_acc, val_acc, test_acc]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Treinar modelo\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gcn = NodeClassifier(dataset.num_features, 16, dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), \n",
    "    lr=0.01, weight_decay=5e-4\n",
    ")\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(gcn, optimizer, data)\n",
    "    if epoch % 20 == 0:\n",
    "        train_acc, val_acc, test_acc = test(gcn, data)\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Salvar Modelo \n",
    "# -----------------------------\n",
    "import os, json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "ckpt_gcn = \"../models/gcn_state.pt\"\n",
    "ckpt_meta  = \"../models/meta.pt\"\n",
    "ckpt_graph = \"../models/graph_artifacts.pt\"\n",
    "\n",
    "# Salva pesos\n",
    "torch.save(gcn.state_dict(), ckpt_gcn)\n",
    "\n",
    "# Salva metadados\n",
    "torch.save({\n",
    "    \"num_features\": dataset.num_features,\n",
    "    \"hidden_channels\": 16,\n",
    "    \"num_classes\": dataset.num_classes\n",
    "}, ckpt_meta)\n",
    "\n",
    "# Salva artefatos do grafo\n",
    "torch.save({\n",
    "    \"x\": data.x.cpu(),\n",
    "    \"edge_index\": data.edge_index.cpu()\n",
    "}, ckpt_graph)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Métricas adicionais \n",
    "# -----------------------------\n",
    "gcn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = gcn(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1).cpu().numpy()\n",
    "    labels = data.y.cpu().numpy()\n",
    "\n",
    "    # Val\n",
    "    val_idx = data.val_mask.cpu().numpy()\n",
    "    val_acc  = accuracy_score(labels[val_idx], pred[val_idx])\n",
    "    val_prec = precision_score(labels[val_idx], pred[val_idx], average=\"macro\", zero_division=0)\n",
    "    val_rec  = recall_score(labels[val_idx], pred[val_idx], average=\"macro\", zero_division=0)\n",
    "    val_f1   = f1_score(labels[val_idx], pred[val_idx], average=\"macro\", zero_division=0)\n",
    "\n",
    "    # Test\n",
    "    test_idx = data.test_mask.cpu().numpy()\n",
    "    test_acc  = accuracy_score(labels[test_idx], pred[test_idx])\n",
    "    test_prec = precision_score(labels[test_idx], pred[test_idx], average=\"macro\", zero_division=0)\n",
    "    test_rec  = recall_score(labels[test_idx], pred[test_idx], average=\"macro\", zero_division=0)\n",
    "    test_f1   = f1_score(labels[test_idx], pred[test_idx], average=\"macro\", zero_division=0)\n",
    "\n",
    "# Monta dicionário e salva em JSON\n",
    "results = {\n",
    "    \"val\": {\n",
    "        \"accuracy\": float(val_acc),\n",
    "        \"precision\": float(val_prec),\n",
    "        \"recall\": float(val_rec),\n",
    "        \"f1\": float(val_f1),\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"accuracy\": float(test_acc),\n",
    "        \"precision\": float(test_prec),\n",
    "        \"recall\": float(test_rec),\n",
    "        \"f1\": float(test_f1),\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "with open(\"../results/result.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
