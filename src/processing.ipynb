{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cc3efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 | Loss: 1.1087 | Test Acc: 0.3833 | Test F1: 0.1847\n",
      "Epoch 010 | Loss: 1.0980 | Test Acc: 0.2944 | Test F1: 0.1516\n",
      "Epoch 015 | Loss: 1.0988 | Test Acc: 0.2944 | Test F1: 0.1621\n",
      "Epoch 020 | Loss: 1.0975 | Test Acc: 0.3278 | Test F1: 0.1760\n",
      "Epoch 025 | Loss: 1.0964 | Test Acc: 0.3056 | Test F1: 0.2241\n",
      "Epoch 030 | Loss: 1.0960 | Test Acc: 0.2833 | Test F1: 0.1478\n",
      "Epoch 035 | Loss: 1.0951 | Test Acc: 0.2833 | Test F1: 0.1736\n",
      "Epoch 040 | Loss: 1.0937 | Test Acc: 0.3167 | Test F1: 0.2526\n",
      "Epoch 045 | Loss: 1.0921 | Test Acc: 0.2833 | Test F1: 0.2287\n",
      "Epoch 050 | Loss: 1.0904 | Test Acc: 0.3000 | Test F1: 0.2369\n",
      "Epoch 055 | Loss: 1.0881 | Test Acc: 0.2944 | Test F1: 0.2396\n",
      "Epoch 060 | Loss: 1.0854 | Test Acc: 0.3111 | Test F1: 0.2533\n",
      "Epoch 065 | Loss: 1.0819 | Test Acc: 0.2889 | Test F1: 0.2625\n",
      "Epoch 070 | Loss: 1.0792 | Test Acc: 0.2944 | Test F1: 0.2628\n",
      "Epoch 075 | Loss: 1.0771 | Test Acc: 0.2944 | Test F1: 0.2677\n",
      "Epoch 080 | Loss: 1.0743 | Test Acc: 0.2889 | Test F1: 0.2581\n",
      "Epoch 085 | Loss: 1.0714 | Test Acc: 0.3000 | Test F1: 0.2706\n",
      "Epoch 090 | Loss: 1.0683 | Test Acc: 0.2778 | Test F1: 0.2524\n",
      "Epoch 095 | Loss: 1.0645 | Test Acc: 0.2889 | Test F1: 0.2728\n",
      "Epoch 100 | Loss: 1.0602 | Test Acc: 0.2944 | Test F1: 0.2807\n",
      "Epoch 105 | Loss: 1.0563 | Test Acc: 0.3111 | Test F1: 0.2982\n",
      "Epoch 110 | Loss: 1.0527 | Test Acc: 0.3056 | Test F1: 0.2933\n",
      "Epoch 115 | Loss: 1.0496 | Test Acc: 0.2889 | Test F1: 0.2680\n",
      "Epoch 120 | Loss: 1.0457 | Test Acc: 0.3056 | Test F1: 0.2912\n",
      "Epoch 125 | Loss: 1.0425 | Test Acc: 0.2889 | Test F1: 0.2767\n",
      "Epoch 130 | Loss: 1.0396 | Test Acc: 0.3056 | Test F1: 0.2988\n",
      "Epoch 135 | Loss: 1.0363 | Test Acc: 0.3167 | Test F1: 0.3047\n",
      "Epoch 140 | Loss: 1.0321 | Test Acc: 0.3000 | Test F1: 0.2940\n",
      "Epoch 145 | Loss: 1.0276 | Test Acc: 0.3056 | Test F1: 0.2947\n",
      "Epoch 150 | Loss: 1.0237 | Test Acc: 0.2833 | Test F1: 0.2718\n",
      "Epoch 155 | Loss: 1.0199 | Test Acc: 0.2833 | Test F1: 0.2719\n",
      "Epoch 160 | Loss: 1.0163 | Test Acc: 0.2722 | Test F1: 0.2624\n",
      "Epoch 165 | Loss: 1.0129 | Test Acc: 0.2556 | Test F1: 0.2458\n",
      "Epoch 170 | Loss: 1.0098 | Test Acc: 0.2667 | Test F1: 0.2550\n",
      "Epoch 175 | Loss: 1.0069 | Test Acc: 0.2667 | Test F1: 0.2567\n",
      "Epoch 180 | Loss: 1.0043 | Test Acc: 0.2889 | Test F1: 0.2802\n",
      "Epoch 185 | Loss: 1.0018 | Test Acc: 0.2778 | Test F1: 0.2709\n",
      "Epoch 190 | Loss: 0.9991 | Test Acc: 0.2833 | Test F1: 0.2753\n",
      "Epoch 195 | Loss: 0.9952 | Test Acc: 0.2944 | Test F1: 0.2873\n",
      "Epoch 200 | Loss: 0.9929 | Test Acc: 0.3000 | Test F1: 0.2946\n",
      "Epoch 205 | Loss: 0.9898 | Test Acc: 0.2889 | Test F1: 0.2826\n",
      "Epoch 210 | Loss: 0.9879 | Test Acc: 0.3000 | Test F1: 0.2945\n",
      "Epoch 215 | Loss: 0.9859 | Test Acc: 0.3056 | Test F1: 0.3015\n",
      "Epoch 220 | Loss: 0.9846 | Test Acc: 0.2944 | Test F1: 0.2888\n",
      "Epoch 225 | Loss: 0.9835 | Test Acc: 0.3056 | Test F1: 0.2984\n",
      "Epoch 230 | Loss: 0.9825 | Test Acc: 0.3111 | Test F1: 0.3051\n",
      "Epoch 235 | Loss: 0.9816 | Test Acc: 0.3056 | Test F1: 0.3003\n",
      "Epoch 240 | Loss: 0.9809 | Test Acc: 0.3111 | Test F1: 0.3051\n",
      "Epoch 245 | Loss: 0.9802 | Test Acc: 0.3111 | Test F1: 0.3051\n",
      "Epoch 250 | Loss: 0.9802 | Test Acc: 0.3000 | Test F1: 0.2934\n",
      "Epoch 255 | Loss: 0.9790 | Test Acc: 0.3167 | Test F1: 0.3096\n",
      "Epoch 260 | Loss: 0.9779 | Test Acc: 0.3056 | Test F1: 0.2987\n",
      "Epoch 265 | Loss: 0.9770 | Test Acc: 0.3000 | Test F1: 0.2946\n",
      "Epoch 270 | Loss: 0.9772 | Test Acc: 0.3000 | Test F1: 0.2930\n",
      "Epoch 275 | Loss: 0.9759 | Test Acc: 0.3056 | Test F1: 0.3000\n",
      "Epoch 280 | Loss: 0.9752 | Test Acc: 0.3167 | Test F1: 0.3103\n",
      "Epoch 285 | Loss: 0.9747 | Test Acc: 0.2833 | Test F1: 0.2756\n",
      "Epoch 290 | Loss: 0.9737 | Test Acc: 0.2833 | Test F1: 0.2745\n",
      "Epoch 295 | Loss: 0.9738 | Test Acc: 0.3056 | Test F1: 0.2950\n",
      "Epoch 300 | Loss: 0.9737 | Test Acc: 0.3111 | Test F1: 0.3040\n",
      "Epoch 305 | Loss: 0.9717 | Test Acc: 0.2944 | Test F1: 0.2902\n",
      "Epoch 310 | Loss: 0.9703 | Test Acc: 0.3000 | Test F1: 0.2949\n",
      "Epoch 315 | Loss: 0.9694 | Test Acc: 0.2833 | Test F1: 0.2811\n",
      "Epoch 320 | Loss: 0.9687 | Test Acc: 0.2833 | Test F1: 0.2807\n",
      "Epoch 325 | Loss: 0.9667 | Test Acc: 0.3111 | Test F1: 0.3070\n",
      "Epoch 330 | Loss: 0.9660 | Test Acc: 0.3056 | Test F1: 0.3021\n",
      "Epoch 335 | Loss: 0.9651 | Test Acc: 0.3111 | Test F1: 0.3082\n",
      "Epoch 340 | Loss: 0.9642 | Test Acc: 0.3056 | Test F1: 0.3006\n",
      "Epoch 345 | Loss: 0.9628 | Test Acc: 0.3000 | Test F1: 0.2939\n",
      "Epoch 350 | Loss: 0.9621 | Test Acc: 0.3056 | Test F1: 0.2992\n",
      "Epoch 355 | Loss: 0.9615 | Test Acc: 0.2889 | Test F1: 0.2841\n",
      "Epoch 360 | Loss: 0.9609 | Test Acc: 0.3111 | Test F1: 0.3043\n",
      "Epoch 365 | Loss: 0.9611 | Test Acc: 0.3111 | Test F1: 0.3043\n",
      "Epoch 370 | Loss: 0.9601 | Test Acc: 0.3000 | Test F1: 0.2945\n",
      "Epoch 375 | Loss: 0.9599 | Test Acc: 0.3056 | Test F1: 0.2992\n",
      "Epoch 380 | Loss: 0.9600 | Test Acc: 0.3000 | Test F1: 0.2945\n",
      "Epoch 385 | Loss: 0.9601 | Test Acc: 0.3000 | Test F1: 0.2942\n",
      "Epoch 390 | Loss: 0.9585 | Test Acc: 0.2889 | Test F1: 0.2842\n",
      "Epoch 395 | Loss: 0.9600 | Test Acc: 0.2889 | Test F1: 0.2846\n",
      "Epoch 400 | Loss: 0.9583 | Test Acc: 0.2944 | Test F1: 0.2891\n",
      "Epoch 405 | Loss: 0.9580 | Test Acc: 0.3000 | Test F1: 0.2943\n",
      "Epoch 410 | Loss: 0.9582 | Test Acc: 0.2833 | Test F1: 0.2791\n",
      "Epoch 415 | Loss: 0.9573 | Test Acc: 0.3056 | Test F1: 0.2987\n",
      "Epoch 420 | Loss: 0.9571 | Test Acc: 0.3000 | Test F1: 0.2958\n",
      "Epoch 425 | Loss: 0.9563 | Test Acc: 0.2944 | Test F1: 0.2881\n",
      "Epoch 430 | Loss: 0.9568 | Test Acc: 0.3000 | Test F1: 0.2974\n",
      "Epoch 435 | Loss: 0.9561 | Test Acc: 0.3056 | Test F1: 0.2968\n",
      "Epoch 440 | Loss: 0.9578 | Test Acc: 0.3056 | Test F1: 0.3006\n",
      "Epoch 445 | Loss: 0.9568 | Test Acc: 0.2889 | Test F1: 0.2843\n",
      "Epoch 450 | Loss: 0.9557 | Test Acc: 0.3111 | Test F1: 0.3037\n",
      "Epoch 455 | Loss: 0.9554 | Test Acc: 0.3056 | Test F1: 0.2992\n",
      "Epoch 460 | Loss: 0.9548 | Test Acc: 0.2833 | Test F1: 0.2799\n",
      "Epoch 465 | Loss: 0.9545 | Test Acc: 0.2944 | Test F1: 0.2891\n",
      "Epoch 470 | Loss: 0.9541 | Test Acc: 0.3167 | Test F1: 0.3088\n",
      "Epoch 475 | Loss: 0.9545 | Test Acc: 0.3056 | Test F1: 0.2993\n",
      "Epoch 480 | Loss: 0.9544 | Test Acc: 0.2944 | Test F1: 0.2893\n",
      "Epoch 485 | Loss: 0.9532 | Test Acc: 0.2944 | Test F1: 0.2892\n",
      "Epoch 490 | Loss: 0.9531 | Test Acc: 0.2889 | Test F1: 0.2803\n",
      "Epoch 495 | Loss: 0.9533 | Test Acc: 0.2833 | Test F1: 0.2775\n",
      "Modelo salvo em ../models/gcn_state.json\n",
      "Modelo salvo em ../models/predictor_state.json\n",
      "Modelo salvo em ../models/meta.json\n",
      "Modelo salvo em ../models/graph_artifacts.json\n",
      "Métricas salvas em ../results/result.json\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 0. Carregar Dependências\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Carregar dataset CSV\n",
    "# -----------------------------\n",
    "nodes = pd.read_csv(\"../datasets/radnet_synthetic_nodes.csv\", index_col=0)\n",
    "edges = pd.read_csv(\"../datasets/radnet_synthetic_edges.csv\")\n",
    "\n",
    "# tensores\n",
    "x = torch.tensor(nodes.values, dtype=torch.float)\n",
    "edge_index = torch.tensor([edges[\"src\"].values, edges[\"dst\"].values], dtype=torch.long)\n",
    "\n",
    "label_map = {\"baixa\": 0, \"media\": 1, \"alta\": 2}\n",
    "y = torch.tensor([label_map[s] for s in edges[\"label\"].values], dtype=torch.long)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. Divisão treino/teste parametrizável\n",
    "# -------------------------------------------\n",
    "test_size = 0.3        # fração para teste\n",
    "random_state = 42      # semente para reprodutibilidade\n",
    "\n",
    "all_idx = np.arange(edge_index.size(1))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    all_idx,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_idx  = torch.tensor(test_idx,  dtype=torch.long)\n",
    "\n",
    "train_edges = edge_index[:, train_idx]\n",
    "test_edges  = edge_index[:, test_idx]\n",
    "train_labels = y[train_idx]\n",
    "test_labels  = y[test_idx]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Definir modelos\n",
    "# -----------------------------\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        h = torch.matmul(adj, x)   # agregação\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = torch.matmul(adj, h)\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_channels * 2, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x_i, x_j):\n",
    "        z = torch.cat([x_i, x_j], dim=-1)\n",
    "        z = F.relu(self.lin1(z))\n",
    "        return self.lin2(z)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preparar matriz de adjacência\n",
    "# -----------------------------\n",
    "num_nodes = x.size(0)\n",
    "adj = torch.eye(num_nodes)\n",
    "for i, j in zip(edges[\"src\"], edges[\"dst\"]):\n",
    "    adj[i, j] = 1\n",
    "\n",
    "# normalização\n",
    "deg = adj.sum(1)\n",
    "deg_inv = torch.pow(deg, -0.5)\n",
    "deg_inv[deg_inv == float(\"inf\")] = 0\n",
    "D_inv = torch.diag(deg_inv)\n",
    "adj_norm = D_inv @ adj @ D_inv\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Treino e avaliação\n",
    "# -----------------------------\n",
    "gcn = SimpleGCN(in_channels=x.size(1), hidden_channels=8, out_channels=4)\n",
    "predictor = LinkPredictor(in_channels=4, hidden_channels=8, num_classes=3)\n",
    "optimizer = torch.optim.Adam(list(gcn.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "    # treino\n",
    "    gcn.train(); predictor.train()\n",
    "    z = gcn(x, adj_norm)\n",
    "    preds = []\n",
    "    for i, j in zip(train_edges[0], train_edges[1]):\n",
    "        preds.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    \n",
    "    loss = F.cross_entropy(preds, train_labels)\n",
    "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "    \n",
    "    # teste periódico\n",
    "    if epoch % 5 == 0:\n",
    "        gcn.eval(); predictor.eval()\n",
    "        z = gcn(x, adj_norm)\n",
    "        preds = []\n",
    "        for i, j in zip(test_edges[0], test_edges[1]):\n",
    "            preds.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        \n",
    "        y_true = test_labels.numpy()\n",
    "        y_pred = preds.argmax(dim=-1).detach().numpy()\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        \n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f} | Test F1: {f1:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Salvar modelo treinado\n",
    "# -----------------------------\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "ckpt_gcn = \"../models/gcn_state.pt\"\n",
    "ckpt_pred = \"../models/predictor_state.pt\"\n",
    "ckpt_meta = \"../models/meta.pt\"\n",
    "ckpt_graph = \"../models/graph_artifacts.pt\"\n",
    "\n",
    "# Salva pesos\n",
    "torch.save(gcn.state_dict(), ckpt_gcn)\n",
    "torch.save(predictor.state_dict(), ckpt_pred)\n",
    "\n",
    "# Salva metadados\n",
    "torch.save({\n",
    "    \"num_features\": x.size(1),   # nº de features do nó (aqui 3)\n",
    "    \"gcn_hidden\": 8,             # tamanho da hidden layer do GCN\n",
    "    \"gcn_out\": 4,                # saída do GCN\n",
    "    \"pred_hidden\": 8,            # hidden layer do LinkPredictor\n",
    "    \"num_classes\": 3             # nº de classes (baixa, media, alta)\n",
    "}, ckpt_meta)\n",
    "\n",
    "# Salva artefatos do grafo\n",
    "torch.save({\n",
    "    \"x\": x.cpu(),              # features dos nós\n",
    "    \"adj_norm\": adj_norm.cpu() # adjacência normalizada\n",
    "}, ckpt_graph)\n",
    "\n",
    "print(\"Modelo salvo em ../models/gcn_state.json\")\n",
    "print(\"Modelo salvo em ../models/predictor_state.json\")\n",
    "print(\"Modelo salvo em ../models/meta.json\")\n",
    "print(\"Modelo salvo em ../models/graph_artifacts.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Métricas adicionais \n",
    "# -----------------------------\n",
    "import os, json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# nomes de classes (na mesma ordem do mapeamento usado ao carregar os rótulos)\n",
    "class_names = [\"baixa\", \"media\", \"alta\"]\n",
    "\n",
    "gcn.eval(); predictor.eval()\n",
    "with torch.no_grad():\n",
    "    # embeddings dos nós\n",
    "    z = gcn(x, adj_norm)\n",
    "\n",
    "    def eval_split(edges_tensor, labels_tensor):\n",
    "        # logits para cada aresta do split\n",
    "        logits = []\n",
    "        for i, j in zip(edges_tensor[0], edges_tensor[1]):\n",
    "            logits.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "        logits = torch.cat(logits, dim=0)\n",
    "\n",
    "        y_true = labels_tensor.cpu().numpy()\n",
    "        y_pred = logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        # métricas agregadas\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec_macro  = precision_score(y_true, y_pred, average=\"macro\",  zero_division=0)\n",
    "        rec_macro   = recall_score(y_true,    y_pred, average=\"macro\",  zero_division=0)\n",
    "        f1_macro    = f1_score(y_true,        y_pred, average=\"macro\",  zero_division=0)\n",
    "\n",
    "        prec_weight = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "        rec_weight  = recall_score(y_true,    y_pred, average=\"weighted\", zero_division=0)\n",
    "        f1_weight   = f1_score(y_true,        y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        # métricas por classe (mesma ordem de class_names)\n",
    "        prec_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        rec_per_class  = recall_score(y_true,    y_pred, average=None, zero_division=0)\n",
    "        f1_per_class   = f1_score(y_true,        y_pred, average=None, zero_division=0)\n",
    "\n",
    "        per_class = {\n",
    "            cname: {\n",
    "                \"precision\": float(prec_per_class[idx]),\n",
    "                \"recall\":    float(rec_per_class[idx]),\n",
    "                \"f1\":        float(f1_per_class[idx]),\n",
    "            }\n",
    "            for idx, cname in enumerate(class_names)\n",
    "        }\n",
    "\n",
    "        aggregate = {\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision_macro\":  float(prec_macro),\n",
    "            \"recall_macro\":     float(rec_macro),\n",
    "            \"f1_macro\":         float(f1_macro),\n",
    "            \"precision_weighted\": float(prec_weight),\n",
    "            \"recall_weighted\":    float(rec_weight),\n",
    "            \"f1_weighted\":        float(f1_weight),\n",
    "        }\n",
    "\n",
    "        return aggregate, per_class\n",
    "\n",
    "    train_agg, train_per_class = eval_split(train_edges, train_labels)\n",
    "    test_agg,  test_per_class  = eval_split(test_edges,  test_labels)\n",
    "\n",
    "# Monta dicionário e salva em JSON\n",
    "results = {\n",
    "    \"task\": \"edge_load_classification\",\n",
    "    \"classes\": class_names,\n",
    "    \"train\": {\n",
    "        **train_agg,\n",
    "        \"per_class\": train_per_class,\n",
    "    },\n",
    "    \"test\": {\n",
    "        **test_agg,\n",
    "        \"per_class\": test_per_class,\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "with open(\"../results/result.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Métricas salvas em ../results/result.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
