{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc3efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 | Loss: 1.1018 | Test Acc: 0.3833 | Test F1: 0.1847\n",
      "Epoch 010 | Loss: 1.0984 | Test Acc: 0.2889 | Test F1: 0.2236\n",
      "Epoch 015 | Loss: 1.0982 | Test Acc: 0.2944 | Test F1: 0.1516\n",
      "Epoch 020 | Loss: 1.0964 | Test Acc: 0.2944 | Test F1: 0.1516\n",
      "Epoch 025 | Loss: 1.0959 | Test Acc: 0.3111 | Test F1: 0.2183\n",
      "Epoch 030 | Loss: 1.0939 | Test Acc: 0.2889 | Test F1: 0.2255\n",
      "Epoch 035 | Loss: 1.0913 | Test Acc: 0.3000 | Test F1: 0.2356\n",
      "Epoch 040 | Loss: 1.0878 | Test Acc: 0.2778 | Test F1: 0.2266\n",
      "Epoch 045 | Loss: 1.0843 | Test Acc: 0.2889 | Test F1: 0.2635\n",
      "Epoch 050 | Loss: 1.0813 | Test Acc: 0.3000 | Test F1: 0.2996\n",
      "Epoch 055 | Loss: 1.0791 | Test Acc: 0.2833 | Test F1: 0.2738\n",
      "Epoch 060 | Loss: 1.0769 | Test Acc: 0.3000 | Test F1: 0.2837\n",
      "Epoch 065 | Loss: 1.0741 | Test Acc: 0.2944 | Test F1: 0.2763\n",
      "Epoch 070 | Loss: 1.0710 | Test Acc: 0.3222 | Test F1: 0.3076\n",
      "Epoch 075 | Loss: 1.0673 | Test Acc: 0.3222 | Test F1: 0.3137\n",
      "Epoch 080 | Loss: 1.0631 | Test Acc: 0.3111 | Test F1: 0.3011\n",
      "Epoch 085 | Loss: 1.0589 | Test Acc: 0.3222 | Test F1: 0.3094\n",
      "Epoch 090 | Loss: 1.0552 | Test Acc: 0.3056 | Test F1: 0.2937\n",
      "Epoch 095 | Loss: 1.0499 | Test Acc: 0.3056 | Test F1: 0.2799\n",
      "Epoch 100 | Loss: 1.0450 | Test Acc: 0.3167 | Test F1: 0.3001\n",
      "Epoch 105 | Loss: 1.0407 | Test Acc: 0.3000 | Test F1: 0.2759\n",
      "Epoch 110 | Loss: 1.0358 | Test Acc: 0.2889 | Test F1: 0.2526\n",
      "Epoch 115 | Loss: 1.0306 | Test Acc: 0.2889 | Test F1: 0.2500\n",
      "Epoch 120 | Loss: 1.0251 | Test Acc: 0.2889 | Test F1: 0.2443\n",
      "Epoch 125 | Loss: 1.0177 | Test Acc: 0.3056 | Test F1: 0.2719\n",
      "Epoch 130 | Loss: 1.0104 | Test Acc: 0.3222 | Test F1: 0.3016\n",
      "Epoch 135 | Loss: 1.0042 | Test Acc: 0.3222 | Test F1: 0.2993\n",
      "Epoch 140 | Loss: 0.9970 | Test Acc: 0.3111 | Test F1: 0.2940\n",
      "Epoch 145 | Loss: 0.9915 | Test Acc: 0.3167 | Test F1: 0.3006\n",
      "Epoch 150 | Loss: 0.9851 | Test Acc: 0.3222 | Test F1: 0.2855\n",
      "Epoch 155 | Loss: 0.9800 | Test Acc: 0.3389 | Test F1: 0.3193\n",
      "Epoch 160 | Loss: 0.9752 | Test Acc: 0.3056 | Test F1: 0.3002\n",
      "Epoch 165 | Loss: 0.9702 | Test Acc: 0.3167 | Test F1: 0.2849\n",
      "Epoch 170 | Loss: 0.9641 | Test Acc: 0.3056 | Test F1: 0.2695\n",
      "Epoch 175 | Loss: 0.9587 | Test Acc: 0.3056 | Test F1: 0.2743\n",
      "Epoch 180 | Loss: 0.9529 | Test Acc: 0.3056 | Test F1: 0.2733\n",
      "Epoch 185 | Loss: 0.9472 | Test Acc: 0.3000 | Test F1: 0.2971\n",
      "Epoch 190 | Loss: 0.9413 | Test Acc: 0.3000 | Test F1: 0.2972\n",
      "Epoch 195 | Loss: 0.9370 | Test Acc: 0.3056 | Test F1: 0.3024\n",
      "Epoch 200 | Loss: 0.9325 | Test Acc: 0.3111 | Test F1: 0.3075\n",
      "Epoch 205 | Loss: 0.9290 | Test Acc: 0.3111 | Test F1: 0.3073\n",
      "Epoch 210 | Loss: 0.9251 | Test Acc: 0.3222 | Test F1: 0.3172\n",
      "Epoch 215 | Loss: 0.9213 | Test Acc: 0.3000 | Test F1: 0.2958\n",
      "Epoch 220 | Loss: 0.9182 | Test Acc: 0.2944 | Test F1: 0.2907\n",
      "Epoch 225 | Loss: 0.9175 | Test Acc: 0.3056 | Test F1: 0.2962\n",
      "Epoch 230 | Loss: 0.9158 | Test Acc: 0.2889 | Test F1: 0.2832\n",
      "Epoch 235 | Loss: 0.9108 | Test Acc: 0.2944 | Test F1: 0.2915\n",
      "Epoch 240 | Loss: 0.9114 | Test Acc: 0.2778 | Test F1: 0.2517\n",
      "Epoch 245 | Loss: 0.9062 | Test Acc: 0.2833 | Test F1: 0.2609\n",
      "Epoch 250 | Loss: 0.9044 | Test Acc: 0.2889 | Test F1: 0.2653\n",
      "Epoch 255 | Loss: 0.9026 | Test Acc: 0.2889 | Test F1: 0.2653\n",
      "Epoch 260 | Loss: 0.9008 | Test Acc: 0.2889 | Test F1: 0.2654\n",
      "Epoch 265 | Loss: 0.8996 | Test Acc: 0.2833 | Test F1: 0.2575\n",
      "Epoch 270 | Loss: 0.8982 | Test Acc: 0.2778 | Test F1: 0.2537\n",
      "Epoch 275 | Loss: 0.8973 | Test Acc: 0.2833 | Test F1: 0.2649\n",
      "Epoch 280 | Loss: 0.8954 | Test Acc: 0.2778 | Test F1: 0.2594\n",
      "Epoch 285 | Loss: 0.8938 | Test Acc: 0.2944 | Test F1: 0.2850\n",
      "Epoch 290 | Loss: 0.8920 | Test Acc: 0.2889 | Test F1: 0.2806\n",
      "Epoch 295 | Loss: 0.8911 | Test Acc: 0.3111 | Test F1: 0.3013\n",
      "Epoch 300 | Loss: 0.8899 | Test Acc: 0.3167 | Test F1: 0.3060\n",
      "Epoch 305 | Loss: 0.8875 | Test Acc: 0.3111 | Test F1: 0.3008\n",
      "Epoch 310 | Loss: 0.8850 | Test Acc: 0.3111 | Test F1: 0.3011\n",
      "Epoch 315 | Loss: 0.8858 | Test Acc: 0.3111 | Test F1: 0.3010\n",
      "Epoch 320 | Loss: 0.8861 | Test Acc: 0.3111 | Test F1: 0.3010\n",
      "Epoch 325 | Loss: 0.8824 | Test Acc: 0.3167 | Test F1: 0.3055\n",
      "Epoch 330 | Loss: 0.8822 | Test Acc: 0.3167 | Test F1: 0.3077\n",
      "Epoch 335 | Loss: 0.8791 | Test Acc: 0.3167 | Test F1: 0.3077\n",
      "Epoch 340 | Loss: 0.8783 | Test Acc: 0.3167 | Test F1: 0.3078\n",
      "Epoch 345 | Loss: 0.8766 | Test Acc: 0.3167 | Test F1: 0.3069\n",
      "Epoch 350 | Loss: 0.8752 | Test Acc: 0.3167 | Test F1: 0.3069\n",
      "Epoch 355 | Loss: 0.8759 | Test Acc: 0.3278 | Test F1: 0.3146\n",
      "Epoch 360 | Loss: 0.8743 | Test Acc: 0.3389 | Test F1: 0.3236\n",
      "Epoch 365 | Loss: 0.8739 | Test Acc: 0.3333 | Test F1: 0.3210\n",
      "Epoch 370 | Loss: 0.8727 | Test Acc: 0.3222 | Test F1: 0.3118\n",
      "Epoch 375 | Loss: 0.8720 | Test Acc: 0.3167 | Test F1: 0.3050\n",
      "Epoch 380 | Loss: 0.8718 | Test Acc: 0.3222 | Test F1: 0.3097\n",
      "Epoch 385 | Loss: 0.8706 | Test Acc: 0.3222 | Test F1: 0.3081\n",
      "Epoch 390 | Loss: 0.8689 | Test Acc: 0.3278 | Test F1: 0.3150\n",
      "Epoch 395 | Loss: 0.8685 | Test Acc: 0.3111 | Test F1: 0.2990\n",
      "Epoch 400 | Loss: 0.8670 | Test Acc: 0.3167 | Test F1: 0.3050\n",
      "Epoch 405 | Loss: 0.8674 | Test Acc: 0.3111 | Test F1: 0.2975\n",
      "Epoch 410 | Loss: 0.8675 | Test Acc: 0.3111 | Test F1: 0.2990\n",
      "Epoch 415 | Loss: 0.8675 | Test Acc: 0.3167 | Test F1: 0.3050\n",
      "Epoch 420 | Loss: 0.8679 | Test Acc: 0.3111 | Test F1: 0.2991\n",
      "Epoch 425 | Loss: 0.8647 | Test Acc: 0.3167 | Test F1: 0.3036\n",
      "Epoch 430 | Loss: 0.8629 | Test Acc: 0.3222 | Test F1: 0.3108\n",
      "Epoch 435 | Loss: 0.8616 | Test Acc: 0.3167 | Test F1: 0.3001\n",
      "Epoch 440 | Loss: 0.8611 | Test Acc: 0.3222 | Test F1: 0.3085\n",
      "Epoch 445 | Loss: 0.8586 | Test Acc: 0.3278 | Test F1: 0.3154\n",
      "Epoch 450 | Loss: 0.8577 | Test Acc: 0.3222 | Test F1: 0.3045\n",
      "Epoch 455 | Loss: 0.8570 | Test Acc: 0.3167 | Test F1: 0.2996\n",
      "Epoch 460 | Loss: 0.8562 | Test Acc: 0.3167 | Test F1: 0.2996\n",
      "Epoch 465 | Loss: 0.8550 | Test Acc: 0.3222 | Test F1: 0.3060\n",
      "Epoch 470 | Loss: 0.8542 | Test Acc: 0.3222 | Test F1: 0.3085\n",
      "Epoch 475 | Loss: 0.8564 | Test Acc: 0.3056 | Test F1: 0.2891\n",
      "Epoch 480 | Loss: 0.8544 | Test Acc: 0.3111 | Test F1: 0.2964\n",
      "Epoch 485 | Loss: 0.8533 | Test Acc: 0.3222 | Test F1: 0.3055\n",
      "Epoch 490 | Loss: 0.8522 | Test Acc: 0.3056 | Test F1: 0.2891\n",
      "Epoch 495 | Loss: 0.8512 | Test Acc: 0.3222 | Test F1: 0.3051\n",
      "Modelo salvo em ../models/gcn_state.pt\n",
      "Modelo salvo em ../models/predictor_state.pt\n",
      "Modelo salvo em ../models/meta.pt\n",
      "Modelo salvo em ../models/graph_artifacts.pt\n",
      "Métricas salvas em ../results/result.json\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 0. Carregar Dependências\n",
    "# -----------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Carregar dataset CSV\n",
    "# -----------------------------\n",
    "nodes = pd.read_csv(\"../datasets/radnet_synthetic_nodes.csv\", index_col=0)\n",
    "edges = pd.read_csv(\"../datasets/radnet_synthetic_edges.csv\")\n",
    "\n",
    "# tensores\n",
    "x = torch.tensor(nodes.values, dtype=torch.float)\n",
    "edge_index = torch.tensor([edges[\"src\"].values, edges[\"dst\"].values], dtype=torch.long)\n",
    "\n",
    "label_map = {\"baixa\": 0, \"media\": 1, \"alta\": 2}\n",
    "y = torch.tensor([label_map[s] for s in edges[\"label\"].values], dtype=torch.long)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. Divisão treino/teste parametrizável\n",
    "# -------------------------------------------\n",
    "test_size = 0.3        # fração para teste\n",
    "random_state = 42      # semente para reprodutibilidade\n",
    "\n",
    "all_idx = np.arange(edge_index.size(1))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    all_idx,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_idx  = torch.tensor(test_idx,  dtype=torch.long)\n",
    "\n",
    "train_edges = edge_index[:, train_idx]\n",
    "test_edges  = edge_index[:, test_idx]\n",
    "train_labels = y[train_idx]\n",
    "test_labels  = y[test_idx]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Definir modelos\n",
    "# -----------------------------\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        h = torch.matmul(adj, x)   # agregação\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = torch.matmul(adj, h)\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_channels * 2, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x_i, x_j):\n",
    "        z = torch.cat([x_i, x_j], dim=-1)\n",
    "        z = F.relu(self.lin1(z))\n",
    "        return self.lin2(z)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preparar matriz de adjacência\n",
    "# -----------------------------\n",
    "num_nodes = x.size(0)\n",
    "adj = torch.eye(num_nodes)\n",
    "for i, j in zip(edges[\"src\"], edges[\"dst\"]):\n",
    "    adj[i, j] = 1\n",
    "\n",
    "# normalização\n",
    "deg = adj.sum(1)\n",
    "deg_inv = torch.pow(deg, -0.5)\n",
    "deg_inv[deg_inv == float(\"inf\")] = 0\n",
    "D_inv = torch.diag(deg_inv)\n",
    "adj_norm = D_inv @ adj @ D_inv\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Treino e avaliação\n",
    "# -----------------------------\n",
    "gcn = SimpleGCN(in_channels=x.size(1), hidden_channels=8, out_channels=4)\n",
    "predictor = LinkPredictor(in_channels=4, hidden_channels=8, num_classes=3)\n",
    "optimizer = torch.optim.Adam(list(gcn.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "    # treino\n",
    "    gcn.train(); predictor.train()\n",
    "    z = gcn(x, adj_norm)\n",
    "    preds = []\n",
    "    for i, j in zip(train_edges[0], train_edges[1]):\n",
    "        preds.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    \n",
    "    loss = F.cross_entropy(preds, train_labels)\n",
    "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "    \n",
    "    # teste periódico\n",
    "    if epoch % 5 == 0:\n",
    "        gcn.eval(); predictor.eval()\n",
    "        z = gcn(x, adj_norm)\n",
    "        preds = []\n",
    "        for i, j in zip(test_edges[0], test_edges[1]):\n",
    "            preds.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        \n",
    "        y_true = test_labels.numpy()\n",
    "        y_pred = preds.argmax(dim=-1).detach().numpy()\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        \n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f} | Test F1: {f1:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Salvar modelo treinado\n",
    "# -----------------------------\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "ckpt_gcn = \"../models/gcn_state.pt\"\n",
    "ckpt_pred = \"../models/predictor_state.pt\"\n",
    "ckpt_meta = \"../models/meta.pt\"\n",
    "ckpt_graph = \"../models/graph_artifacts.pt\"\n",
    "\n",
    "# Salva pesos\n",
    "torch.save(gcn.state_dict(), ckpt_gcn)\n",
    "torch.save(predictor.state_dict(), ckpt_pred)\n",
    "\n",
    "# Salva metadados\n",
    "torch.save({\n",
    "    \"num_features\": x.size(1),   # nº de features do nó (aqui 3)\n",
    "    \"gcn_hidden\": 8,             # tamanho da hidden layer do GCN\n",
    "    \"gcn_out\": 4,                # saída do GCN\n",
    "    \"pred_hidden\": 8,            # hidden layer do LinkPredictor\n",
    "    \"num_classes\": 3             # nº de classes (baixa, media, alta)\n",
    "}, ckpt_meta)\n",
    "\n",
    "# Salva artefatos do grafo\n",
    "torch.save({\n",
    "    \"x\": x.cpu(),              # features dos nós\n",
    "    \"adj_norm\": adj_norm.cpu() # adjacência normalizada\n",
    "}, ckpt_graph)\n",
    "\n",
    "print(\"Modelo salvo em ../models/gcn_state.pt\")\n",
    "print(\"Modelo salvo em ../models/predictor_state.pt\")\n",
    "print(\"Modelo salvo em ../models/meta.pt\")\n",
    "print(\"Modelo salvo em ../models/graph_artifacts.pt\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Métricas adicionais \n",
    "# -----------------------------\n",
    "import os, json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# nomes de classes (na mesma ordem do mapeamento usado ao carregar os rótulos)\n",
    "class_names = [\"baixa\", \"media\", \"alta\"]\n",
    "\n",
    "gcn.eval(); predictor.eval()\n",
    "with torch.no_grad():\n",
    "    # embeddings dos nós\n",
    "    z = gcn(x, adj_norm)\n",
    "\n",
    "    def eval_split(edges_tensor, labels_tensor):\n",
    "        # logits para cada aresta do split\n",
    "        logits = []\n",
    "        for i, j in zip(edges_tensor[0], edges_tensor[1]):\n",
    "            logits.append(predictor(z[i].unsqueeze(0), z[j].unsqueeze(0)))\n",
    "        logits = torch.cat(logits, dim=0)\n",
    "\n",
    "        y_true = labels_tensor.cpu().numpy()\n",
    "        y_pred = logits.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        # métricas agregadas\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec_macro  = precision_score(y_true, y_pred, average=\"macro\",  zero_division=0)\n",
    "        rec_macro   = recall_score(y_true,    y_pred, average=\"macro\",  zero_division=0)\n",
    "        f1_macro    = f1_score(y_true,        y_pred, average=\"macro\",  zero_division=0)\n",
    "\n",
    "        prec_weight = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "        rec_weight  = recall_score(y_true,    y_pred, average=\"weighted\", zero_division=0)\n",
    "        f1_weight   = f1_score(y_true,        y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        # métricas por classe (mesma ordem de class_names)\n",
    "        prec_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        rec_per_class  = recall_score(y_true,    y_pred, average=None, zero_division=0)\n",
    "        f1_per_class   = f1_score(y_true,        y_pred, average=None, zero_division=0)\n",
    "\n",
    "        per_class = {\n",
    "            cname: {\n",
    "                \"precision\": float(prec_per_class[idx]),\n",
    "                \"recall\":    float(rec_per_class[idx]),\n",
    "                \"f1\":        float(f1_per_class[idx]),\n",
    "            }\n",
    "            for idx, cname in enumerate(class_names)\n",
    "        }\n",
    "\n",
    "        aggregate = {\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision_macro\":  float(prec_macro),\n",
    "            \"recall_macro\":     float(rec_macro),\n",
    "            \"f1_macro\":         float(f1_macro),\n",
    "            \"precision_weighted\": float(prec_weight),\n",
    "            \"recall_weighted\":    float(rec_weight),\n",
    "            \"f1_weighted\":        float(f1_weight),\n",
    "        }\n",
    "\n",
    "        return aggregate, per_class\n",
    "\n",
    "    train_agg, train_per_class = eval_split(train_edges, train_labels)\n",
    "    test_agg,  test_per_class  = eval_split(test_edges,  test_labels)\n",
    "\n",
    "# Monta dicionário e salva em JSON\n",
    "results = {\n",
    "    \"task\": \"edge_load_classification\",\n",
    "    \"classes\": class_names,\n",
    "    \"train\": {\n",
    "        **train_agg,\n",
    "        \"per_class\": train_per_class,\n",
    "    },\n",
    "    \"test\": {\n",
    "        **test_agg,\n",
    "        \"per_class\": test_per_class,\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "with open(\"../results/result.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Métricas salvas em ../results/result.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
